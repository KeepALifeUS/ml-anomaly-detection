"""
üå≤ Isolation Forest Anomaly Detector

Implements Isolation Forest algorithm for anomaly detection.
Uses ensemble of isolation trees to identify anomalies by their isolation difficulty.
Anomalies are easier to isolate and have shorter average path lengths.

Context7 Features:
- Distributed tree ensemble
- Auto-scaling parameters
- Model versioning and persistence
- Real-time inference optimization
"""

import numpy as np
import pandas as pd
from typing import Union, List, Tuple, Optional, Dict, Any
from dataclasses import dataclass
import structlog
import joblib
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import roc_auc_score, precision_recall_curve
import warnings

logger = structlog.get_logger(__name__)

@dataclass
class IsolationForestConfig:
    """Configuration for Isolation Forest detector."""
    n_estimators: int = 200  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
    max_samples: Union[int, float, str] = 'auto'  # –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
    contamination: float = 0.1  # –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π
    max_features: float = 1.0  # –î–æ–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
    bootstrap: bool = False  # Bootstrap sampling
    n_jobs: int = -1  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    random_state: Optional[int] = 42
    warm_start: bool = False  # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    
    # Context7 enterprise features
    model_versioning: bool = True  # –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    auto_scaling: bool = True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler_type: str = 'standard'  # 'standard', 'robust', 'none'
    performance_tracking: bool = True  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    # Crypto-specific parameters
    crypto_optimized: bool = True  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –∫—Ä–∏–ø—Ç–æ –¥–∞–Ω–Ω—ã—Ö
    volatility_aware: bool = True  # –£—á–µ—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏

class IsolationForestDetector:
    """
    Isolation Forest Anomaly Detector –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–Ω—Å–∞–º–±–ª—å –∏–∑–æ–ª–∏—Ä—É—é—â–∏—Ö –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π.
    –ê–Ω–æ–º–∞–ª–∏–∏ –ª–µ–≥—á–µ –∏–∑–æ–ª–∏—Ä—É—é—Ç—Å—è –∏ –∏–º–µ—é—Ç –º–µ–Ω—å—à—É—é —Å—Ä–µ–¥–Ω—é—é –≥–ª—É–±–∏–Ω—É –ø—É—Ç–∏.
    
    Context7 Features:
    - Enterprise-grade model management
    - Distributed processing capability
    - Real-time inference optimization
    - Comprehensive monitoring
    """
    
    def __init__(self, config: Optional[IsolationForestConfig] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Isolation Forest –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        """
        self.config = config or IsolationForestConfig()
        self.fitted = False
        self.model = None
        self.scaler = None
        self._feature_names = None
        self._model_version = "1.0.0"
        self._training_stats = {}
        self._performance_metrics = {}
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self._create_model()
        self._create_scaler()
        
        logger.info(
            "IsolationForestDetector initialized",
            n_estimators=self.config.n_estimators,
            contamination=self.config.contamination,
            max_features=self.config.max_features,
            crypto_optimized=self.config.crypto_optimized,
            model_version=self._model_version
        )
    
    def fit(self, X: Union[np.ndarray, pd.DataFrame, pd.Series], 
            y: Optional[np.ndarray] = None) -> 'IsolationForestDetector':
        """
        –û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            X: –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            y: –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞)
            
        Returns:
            self: –û–±—É—á–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
        """
        try:
            X, feature_names = self._validate_and_prepare_input(X)
            self._feature_names = feature_names
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            self._training_stats = {
                "n_samples": len(X),
                "n_features": X.shape[1],
                "feature_means": np.mean(X, axis=0).tolist(),
                "feature_stds": np.std(X, axis=0).tolist()
            }
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if self.scaler is not None:
                logger.info("Scaling features", scaler_type=self.config.scaler_type)
                X_scaled = self.scaler.fit_transform(X)
            else:
                X_scaled = X
            
            # Crypto-specific –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if self.config.crypto_optimized:
                self._optimize_for_crypto_data(X_scaled)
            
            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            logger.info("Training Isolation Forest model")
            self.model.fit(X_scaled)
            
            self.fitted = True
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            if y is not None:
                self._evaluate_performance(X_scaled, y)
            
            logger.info(
                "IsolationForestDetector fitted successfully",
                **self._training_stats,
                model_version=self._model_version,
                crypto_optimized=self.config.crypto_optimized
            )
            
            return self
            
        except Exception as e:
            logger.error("Failed to fit IsolationForestDetector", error=str(e))
            raise
    
    def detect(self, X: Union[np.ndarray, pd.DataFrame, pd.Series]) -> Tuple[np.ndarray, np.ndarray]:
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            X: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: (anomaly_labels, anomaly_scores)
                anomaly_labels: 1 –¥–ª—è –∞–Ω–æ–º–∞–ª–∏–π, 0 –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ç–æ—á–µ–∫
                anomaly_scores: –û—Ü–µ–Ω–∫–∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–∞—è —Ç–æ—á–∫–∞)
        """
        if not self.fitted:
            raise ValueError("Detector must be fitted before detecting anomalies")
        
        try:
            X, _ = self._validate_and_prepare_input(X)
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            if self.scaler is not None:
                X_scaled = self.scaler.transform(X)
            else:
                X_scaled = X
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
            anomaly_labels = self.model.predict(X_scaled)
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º -1/1 –≤ 0/1
            anomaly_labels = np.where(anomaly_labels == -1, 1, 0)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏
            anomaly_scores = self.model.decision_function(X_scaled)
            # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫–∏ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –±–æ–ª–µ–µ –∞–Ω–æ–º–∞–ª—å–Ω–æ)
            anomaly_scores = -anomaly_scores
            
            n_anomalies = np.sum(anomaly_labels)
            
            logger.debug(
                "Isolation Forest detection completed",
                n_samples=len(X),
                n_anomalies=n_anomalies,
                anomaly_rate=f"{np.mean(anomaly_labels):.3%}",
                min_score=np.min(anomaly_scores),
                max_score=np.max(anomaly_scores)
            )
            
            return anomaly_labels, anomaly_scores
            
        except Exception as e:
            logger.error("Failed to detect anomalies with Isolation Forest", error=str(e))
            raise
    
    def predict_proba(self, X: Union[np.ndarray, pd.DataFrame, pd.Series]) -> np.ndarray:
        """
        –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞.
        
        Args:
            X: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            np.ndarray: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∞–Ω–æ–º–∞–ª—å–Ω–æ—Å—Ç–∏ [0, 1]
        """
        if not self.fitted:
            raise ValueError("Detector must be fitted")
        
        try:
            X, _ = self._validate_and_prepare_input(X)
            
            if self.scaler is not None:
                X_scaled = self.scaler.transform(X)
            else:
                X_scaled = X
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫–∏ —Ä–µ—à–µ–Ω–∏—è
            decision_scores = self.model.decision_function(X_scaled)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ [0, 1]
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–≥–º–æ–∏–¥ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
            probabilities = 1 / (1 + np.exp(decision_scores))
            
            return probabilities
            
        except Exception as e:
            logger.error("Failed to compute anomaly probabilities", error=str(e))
            raise
    
    def detect_realtime(self, value: Union[float, np.ndarray, Dict]) -> Tuple[bool, float, float]:
        """
        Real-time –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –¥–ª—è –æ–¥–Ω–æ–π —Ç–æ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            value: –ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            Tuple[bool, float, float]: (is_anomaly, anomaly_score, probability)
        """
        if not self.fitted:
            raise ValueError("Detector must be fitted before real-time detection")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            if isinstance(value, dict):
                if self._feature_names:
                    value = np.array([value[name] for name in self._feature_names])
                else:
                    value = np.array(list(value.values()))
            elif isinstance(value, (int, float)):
                value = np.array([value])
            elif isinstance(value, list):
                value = np.array(value)
            
            value = value.reshape(1, -1)
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
            if self.scaler is not None:
                value_scaled = self.scaler.transform(value)
            else:
                value_scaled = value
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self.model.predict(value_scaled)[0]
            is_anomaly = prediction == -1
            
            # –û—Ü–µ–Ω–∫–∏
            anomaly_score = -self.model.decision_function(value_scaled)[0]
            probability = 1 / (1 + np.exp(-anomaly_score))
            
            if is_anomaly:
                logger.warning(
                    "Real-time Isolation Forest anomaly detected",
                    value=value[0].tolist() if value.ndim > 1 else float(value[0]),
                    anomaly_score=anomaly_score,
                    probability=probability
                )
            
            return is_anomaly, anomaly_score, probability
            
        except Exception as e:
            logger.error("Failed real-time Isolation Forest detection", error=str(e))
            raise
    
    def _validate_and_prepare_input(self, X: Union[np.ndarray, pd.DataFrame, pd.Series]) -> Tuple[np.ndarray, Optional[List[str]]]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
        feature_names = None
        
        if isinstance(X, pd.DataFrame):
            feature_names = X.columns.tolist()
            X = X.values
        elif isinstance(X, pd.Series):
            feature_names = [X.name] if X.name else ["feature_0"]
            X = X.values.reshape(-1, 1)
        elif isinstance(X, list):
            X = np.array(X)
        
        if X.ndim == 1:
            X = X.reshape(-1, 1)
            if not feature_names:
                feature_names = ["feature_0"]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ Inf
        if np.any(~np.isfinite(X)):
            warnings.warn("Input contains NaN or Inf values, replacing with median")
            from sklearn.impute import SimpleImputer
            imputer = SimpleImputer(strategy='median')
            X = imputer.fit_transform(X)
        
        return X, feature_names
    
    def _create_model(self) -> None:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Isolation Forest."""
        self.model = IsolationForest(
            n_estimators=self.config.n_estimators,
            max_samples=self.config.max_samples,
            contamination=self.config.contamination,
            max_features=self.config.max_features,
            bootstrap=self.config.bootstrap,
            n_jobs=self.config.n_jobs,
            random_state=self.config.random_state,
            warm_start=self.config.warm_start,
            verbose=0
        )
    
    def _create_scaler(self) -> None:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        if self.config.scaler_type == 'standard':
            self.scaler = StandardScaler()
        elif self.config.scaler_type == 'robust':
            self.scaler = RobustScaler()
        elif self.config.scaler_type == 'none':
            self.scaler = None
        else:
            logger.warning(
                "Unknown scaler type, using standard",
                scaler_type=self.config.scaler_type
            )
            self.scaler = StandardScaler()
    
    def _optimize_for_crypto_data(self, X: np.ndarray) -> None:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–¥–∞–Ω–Ω—ã—Ö."""
        if not self.config.crypto_optimized:
            return
        
        n_samples, n_features = X.shape
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è contamination –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        if self.config.volatility_aware and n_features > 1:
            volatilities = np.std(X, axis=0)
            avg_volatility = np.mean(volatilities)
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º contamination –¥–ª—è –±–æ–ª–µ–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if avg_volatility > 1.0:
                new_contamination = min(0.2, self.config.contamination * 1.5)
                logger.info(
                    "Adjusting contamination for high volatility crypto data",
                    old_contamination=self.config.contamination,
                    new_contamination=new_contamination,
                    avg_volatility=avg_volatility
                )
                self.model.contamination = new_contamination
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è max_samples –¥–ª—è crypto –¥–∞–Ω–Ω—ã—Ö
        if isinstance(self.config.max_samples, str) and self.config.max_samples == 'auto':
            # –î–ª—è –∫—Ä–∏–ø—Ç–æ–¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–∏–µ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            optimal_samples = min(512, max(64, n_samples // 10))
            logger.info(
                "Optimizing max_samples for crypto data",
                original="auto",
                optimized=optimal_samples,
                n_samples=n_samples
            )
            self.model.max_samples = optimal_samples
    
    def _evaluate_performance(self, X: np.ndarray, y: np.ndarray) -> None:
        """–û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            predictions = self.model.predict(X)
            decision_scores = self.model.decision_function(X)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏
            y_binary = np.where(y == 1, 1, 0)  # –ê–Ω–æ–º–∞–ª–∏–∏ = 1
            pred_binary = np.where(predictions == -1, 1, 0)  # –ò–∑–æ–ª–µ–π—à–Ω —Ñ–æ—Ä–µ—Å—Ç: -1 = –∞–Ω–æ–º–∞–ª–∏—è
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            if len(np.unique(y_binary)) > 1:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±–µ–∏—Ö –∫–ª–∞—Å—Å–æ–≤
                auc_score = roc_auc_score(y_binary, -decision_scores)
                precision, recall, _ = precision_recall_curve(y_binary, -decision_scores)
                auc_pr = np.trapz(precision, recall)
                
                self._performance_metrics = {
                    "auc_roc": auc_score,
                    "auc_pr": auc_pr,
                    "precision_at_contamination": precision[len(precision)//2],  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                    "recall_at_contamination": recall[len(recall)//2]
                }
                
                logger.info(
                    "Model performance evaluation",
                    **self._performance_metrics
                )
            
        except Exception as e:
            logger.warning("Failed to evaluate model performance", error=str(e))
    
    def save_model(self, filepath: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∏—Å–∫."""
        if not self.fitted:
            raise ValueError("Model must be fitted before saving")
        
        try:
            model_data = {
                "model": self.model,
                "scaler": self.scaler,
                "config": self.config,
                "feature_names": self._feature_names,
                "model_version": self._model_version,
                "training_stats": self._training_stats,
                "performance_metrics": self._performance_metrics
            }
            
            joblib.dump(model_data, filepath)
            
            logger.info(
                "Model saved successfully",
                filepath=filepath,
                model_version=self._model_version
            )
            
        except Exception as e:
            logger.error("Failed to save model", filepath=filepath, error=str(e))
            raise
    
    def load_model(self, filepath: str) -> 'IsolationForestDetector':
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞."""
        try:
            model_data = joblib.load(filepath)
            
            self.model = model_data["model"]
            self.scaler = model_data["scaler"]
            self.config = model_data["config"]
            self._feature_names = model_data.get("feature_names")
            self._model_version = model_data.get("model_version", "unknown")
            self._training_stats = model_data.get("training_stats", {})
            self._performance_metrics = model_data.get("performance_metrics", {})
            self.fitted = True
            
            logger.info(
                "Model loaded successfully",
                filepath=filepath,
                model_version=self._model_version
            )
            
            return self
            
        except Exception as e:
            logger.error("Failed to load model", filepath=filepath, error=str(e))
            raise
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞."""
        if not self.fitted:
            return {"status": "not_fitted"}
        
        return {
            "fitted": True,
            "model_version": self._model_version,
            "config": {
                "n_estimators": self.config.n_estimators,
                "contamination": self.config.contamination,
                "max_features": self.config.max_features,
                "crypto_optimized": self.config.crypto_optimized
            },
            "training_stats": self._training_stats,
            "performance_metrics": self._performance_metrics,
            "feature_names": self._feature_names
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫—Ä–∏–ø—Ç–æ—Ç—Ä–µ–π–¥–∏–Ω–≥–∞
def create_crypto_isolation_forest(
    price_data: pd.DataFrame,
    features: Optional[List[str]] = None,
    contamination: float = 0.05,
    n_estimators: int = 200
) -> IsolationForestDetector:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ Isolation Forest –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        price_data: DataFrame —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        features: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        contamination: –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ–ª—è –∞–Ω–æ–º–∞–ª–∏–π
        n_estimators: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π IsolationForestDetector
    """
    if features is None:
        features = ['close', 'volume']
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –µ—Å–ª–∏ –µ—Å—Ç—å
        optional_features = ['returns', 'volatility', 'rsi', 'macd', 'bollinger_upper', 'bollinger_lower']
        for feature in optional_features:
            if feature in price_data.columns:
                features.append(feature)
    
    config = IsolationForestConfig(
        n_estimators=n_estimators,
        contamination=contamination,
        max_features=0.8,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 80% –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞
        crypto_optimized=True,
        volatility_aware=True,
        scaler_type='robust',  # Robust scaler –ª—É—á—à–µ –¥–ª—è –∫—Ä–∏–ø—Ç–æ–¥–∞–Ω–Ω—ã—Ö
        auto_scaling=True
    )
    
    detector = IsolationForestDetector(config)
    detector.fit(price_data[features].dropna())
    
    return detector